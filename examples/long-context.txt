3.2.3.Memory Management
Efficient memory management remains at the forefront of challenges in LLM serving, especially given the inherent memory-intensive nature of transformer architectures. With the growing need for long-sequence inference, the memory footprint of the KV cache stands out as a prime optimization target compared with model weights and the necessary workspace for other activations. As the KV cache memory grows and shrinks dynamically and unpredictably during incremental decoding, the naive approach (e.g., FasterTransformer) pre-allocates a contiguous piece of memory with a maximum sequence length assumption. It wastes memory severely for 1) input batches with varied request lengths and 2) complex decoding scenarios generating multiple output sequences in parallel (e.g., beam search, parallel decoding). vLLM (Kwon et al., 2023) proposes paged attention that partitions the KV cache into non-contiguous memory blocks and significantly improves the batch size as well as throughput. SpecInfer (Miao et al., 2023a) proposes tree attention and depth-first tree traversal to eliminate redundant KV cache allocation for multiple output sequences sharing the same prefix. LightLLM (lig, 2023) takes a more granular token-level memory management mechanism to further diminish memory usage. However, the overheads of such fragmented memory managing mechanisms pose new challenges. Especially for cases where other optimizations are employed to boost the batch size, these fine-grained memory management methods might offer only marginal throughput benefits while substantially amplifying the inference latency. It‚Äôs evident that memory reduction in LLM inference is intricately tied with other algorithmic innovations and system-level optimizations. While some might work well for specific workloads, they might counteract one another, leading to a degraded overall performance. Striking the right balance between memory efficiency and computational performance of LLM inference systems remains an open and pressing challenge in the field.

3.2.4.Request Scheduling
Efficiently scheduling incoming inference requests is crucial for optimizing LLM serving. This section reviews request scheduling algorithms that maximize resource utilization, guarantee response time within latency service level objective (SLO), and handle varying request loads effectively. Request scheduling for LLM serving shares commonalities with general ML serving techniques, as both aim to efficiently manage incoming requests and optimize resource utilization. These common aspects include dynamic batching (Ali et al., 2020), preemption (Han et al., 2022), priority (Ng et al., 2023), swapping (Bai et al., 2020), model selection (Gunasekaran et al., 2022), cost efficiency (Zhang et al., 2019), load balancing and resource allocation (Weng et al., 2022). However, LLM serving also introduces unique challenges due to its distinctive characteristics, such as the massive model size, iterative autoregressive decoding mechanism, unknown variable output length and state management for context information.

Early LLM serving systems (e.g., FasterTransformer over NVIDIA Triton) only support request-level scheduling which is similar to prior approaches. Orca (Yu et al., 2022a) first notices the gap between generative LLMs and the request-level scheduling of previous ML inference systems. Considering the variable output sequence length, it schedules the execution of the engine at the granularity of iteration with a first-come-first-serve (FCFS) order and enables batching a selected set of operations for better hardware utilization. Plenty of following approaches inherit the selective-batching and iteration-level scheduling policy, such as continuous batching in vLLM and RayLLM (ray, 2023) and in-flight batching in TensorRT-LLM (ten, 2023). Moreover, SpecInfer extends to speculative decoding by iteratively selecting a batch of requests to perform one iteration of speculative inference and verification. FastServe (Wu et al., 2023c) concentrates on the job completion time (JCT) and involves iteration-level preemption to prioritize requests with shorter input length, instead of FCFS. SARATHI (Agrawal et al., 2023) targets the pipeline bubbles in distributed inference caused by the initial iteration of varying length input requests. To saturate the GPU compute, it splits the input prompts into uniform chunks and piggybacks the chunk slot with other requests‚Äô decoding iterations if possible, which is also adopted by DeepSpeed-FastGen called Dynamic SplitFuse (dee, 2023a). S
3
 (Jin et al., 2023) involves an output sequence length predictor and helps to schedule more concurrent requests within the GPU memory constraint for larger batch size and higher inference throughput.

3.2.5.Kernel Optimization
In this subsection, we delve into kernel-level optimizations, which target the performance of specific operations within the language model inference pipeline. These optimizations leverage hardware-specific features and software techniques to accelerate critical computation kernels.

‚Ä¢ Kernel fusion: To reduce overheads from kernel launching and memory accessing, kernel fusion is widely adapted by previous DNN frameworks and compilers. Since the backward computation is not required for LLM inference, more kernel fusion chances exist. Several contemporary Transformer inference engines (e.g., FasterTransformer (fas, 2021), TenTrans (Wu et al., 2021), TurboTransformers (Fang et al., 2021), LightSeq (Wang et al., 2020c), ByteTransformer (Zhai et al., 2023)) and compilers (e.g. Welder  (Shi et al., 2023)) propose to fuse 1) GEMMs with the same shape (e.g., the three linear transformations for query, key and value) and 2) Add Bias with the other non-GEMM kernels, such as residual connection, layer normalization and activation functions (e.g., ReLU). Among these, the optimization of fused multi-head attention kernel has been extensively explored and will be discussed in the following aspect.
‚Ä¢ Tailored attention: To make the attention operations run efficiently on a GPU, customizing or tailoring the GPU kernels specifically for the attention calculation is crucial. For example, cuDNN has provided a fused multi-head attention kernel API (cud, 2023). Meanwhile, several implementations have been open-sourced for more performance gains. These can be roughly classified into two categories due to the special autoregressive decoding mechanism. One is for the first iteration (i.e., the initial/prefill/context/prompt phase), which processes all tokens from the input prompt in parallel. For example, xFormers (Lefaudeux et al., 2022) extends the online softmax trick (Rabe and Staats, 2021; Milakov and Gimelshein, 2018; Choi et al., 2022) to the whole attention calculation using CUTLASS (cut, 2023). The other is for the following iterations (i.e., the incremental/decode/generation phase) and the kernel only generates one output token per iteration. For autoregressive decoding, a common practice is to save the previously computed keys and values so that only a single query is required to compute when generating a new token instead of rerunning the entire sequence. The main direction of optimizations in this field is maximizing thread occupancy and minimizing the on-device high-bandwidth memory (HBM) access (i.e., using shared memory or registers (Chen et al., 2021a)). They usually parallelize across the batch size and number of heads dimension (e.g., FasterTransformer) to distribute workloads. Some further enable parallelizing the sequence length dimension by partitioning the KV cache into chunks but require reducing the chunk-wise results at last, such as FlashDecoding (Tri Dao, [n.‚Äâd.]). A subsequent work FlashDecoding++ (Hong et al., 2023) removes such synchronization for partial softmax by introducing a unified maximum value known in advance. It is necessary to select the appropriate parallel dimension based on the workloads for better thread utilization.
‚Ä¢ Sampling optimization: The sampling algorithm selection can greatly influence the LLM generation quality. The default greedy sampling always picks the token with the highest probability. Parallel sampling techniques, such as beam search, decode the approximate optimal sequences efficiently by maintaining a fixed number (i.e., beam width) of top-scoring sequences every iteration. A variety of stochastic sampling techniques (e.g., top-
ùëò
 (Fan et al., 2018), top-
ùëù
 (Holtzman et al., 2019), temperature controlling (Keskar et al., 2019)) have been propose to introduce randomness for more diverse outputs. However, they are still suffering from several practical system challenges. One is the increased memory pressure from redundant KV cache (¬ß3.2.3), and another is the sampling efficiency issue attributed by the large vocabulary of LLM (i.e., tens of thousands). For example, LightSeq (Wang et al., 2020c) provides an efficient hierarchical implementation that divides the vocabulary into 
ùëò
 groups, retrieves candidates within each group using a few GPU instructions and then re-ranks these candidates to obtain the top-
ùëò
 tokens.
‚Ä¢ Variable sequence length: Another unique challenge of LLM inference is that the sequences can vary in both input length and output length, and the latter is unknown in advance. One way to speed up inference is to process multiple sequences in a batch at once (¬ß3.2.4). However, when a batch of sequences has variable input lengths, padding is often used to make them all the same length for batch processing, wasting computational and memory resources. To alleviate some of these inefficiencies, various strategies can be employed. Packing technique (pac, 2020; Zhai et al., 2023) stores the sequences into a continuous memory space without padding and only unpacks before attention calculation. Ragged tensor (Fegade et al., 2022) further supports computation with minimal padding using compiler-generated kernels. Bucketing the sequence into a smaller computation granularity (e.g., chunks (Du et al., 2023)) is also a possible solution to alleviate memory usage of padding tokens. Due to the mixed execution of the initial phase and incremental phase, bucketing input prompts (Agrawal et al., 2023) also brings new challenges to the memory management and request scheduling (¬ß 3.2.4).
‚Ä¢ Automatic compilation: Most existing LLM inference systems utilize vendor-specific libraries as their backend, such as cuBLAS, cuDNN and CUTLASS, which provide optimized kernel implementations. To further improve the inference efficiency, they also take great efforts on optimizing manually-written kernels for specific LLM operators (e.g., attention) over NVIDIA GPUs. Despite of these work, the trend of using automated DNN compilers still exists, such as TVM (i.e., Unity (Sampson et al., 2022), Relax (Lai et al., 2023) and TensorIR (Feng et al., 2023; Ye et al., 2023)), MLIR (Katel et al., 2022), JAX (Frostig et al., 2018), OpenAI Triton (Tillet et al., 2019), TASO (Jia et al., 2019a) and TorchInductor (Wu, 2023). The compilation approach can help discover potentially more efficient operator implementations (e.g., expression derivation (Zheng et al., 2023b)), and more importantly, facilitate adaptation to alternative hardware platforms, including mobile and edge devices, CPUs, DL accelerators, and other types of GPUs (e.g., AMD GPUs and Apple M2 Ultra).

5.Benchmarks
Building a comprehensive and reproducible benchmark for comparing the performance of various LLM serving system like MLPerf (Reddi et al., 2020) is a critical endeavor for both academic and industrial communities in this field. It will not only help LLM users select the right system solutions but also encourage researchers and developers to keep pace with the advanced optimizations. Unfortunately, despite of some prior reports  (ham, 2023; llm, 2023), up to this point, the community has not yet launched a convincing enough benchmark that takes into account all influencing factors. This is mainly because of the numerous evaluation settings, including model configuration, hardware environment, and request load, among others. Testing under a limited number of setting combinations cannot yield conclusions with credibility. For example, certain system optimization techniques can only achieve performance advantages under high or low load conditions, and conversely, they might even be detrimental. Besides, when measuring inference latency, how to exclude additional overheads not related to GPU inference (such as request scheduling overhead, inherent network latency, etc.) due to differences in system design is also a challenging topic. Additionally, a fair benchmark test needs to consider the strict alignment of model output content, which is often overlooked in many tests.

6.Connection with other surveys
Our survey on efficient generative LLM serving and inference complements and extends the scope of existing literature in the field, while maintaining a distinct focus. Among the related works, (Kim et al., 2023b) comes closest in subject matter exploring the design of more general Transformer models and domain-specific accelerators. However, our survey differentiates itself by focusing specifically on generative LLM serving, a nuanced area that has not been the central focus of other studies. Moreover, some studies delve into experimental investigations of LLM inference efficiency on GPUs (Narayanan et al., 2023; Zhang et al., 2023b) and novel accelerators (Emani et al., 2023), offering valuable empirical insights that are directly relevant to our focus on serving efficiency. Additionally, LLMCarbon (Faiz et al., 2023) addresses an increasingly important aspect of LLM deployment ‚Äì its environmental impact (e.g., carbon footprints). While our survey‚Äôs primary focus is efficiency from a performance standpoint, the environmental lens provided by such studies is undeniably relevant and respected in our broader discussion. Some surveys and benchmarks (Jaiswal et al., 2023) offer valuable insights into model compression (Zhu et al., 2023b; Gupta and Agrawal, 2022; Zhu et al., 2023b; Treviso et al., 2023) and quantization (Yao et al., 2023; Gholami et al., 2022). These studies lay a groundwork that indirectly supports our exploration of related directions. Some studies (Muhlgay et al., 2023; Dalvi et al., 2023) provide essential context for understanding LLM effectiveness (e.g., accuracy, perplexity, factuality and so on), which is beyond the scope of this survey. Our survey also acknowledges the contributions of prior surveys (Ben-Nun and Hoefler, 2019; Mayer and Jacobsen, 2020) focusing on distributed training of large-scale DNN models, as they inform the backdrop against which LLM serving must be considered. In essence, our survey situates itself amidst a diverse array of studies, drawing from and contributing to a more holistic understanding of LLM serving efficiency, including both algorithmic innovations and system optimizations. By integrating insights from these various areas, we aim to provide a nuanced and comprehensive overview of the latest advancements and challenges in the field.

7.Future Direction
As we stand at the forefront of LLM advancements, it becomes increasingly important to not only understand the current state of these technologies but also to anticipate and shape their future trajectory. Particularly in the realm of generative LLM serving, there is a vast landscape of unexplored possibilities and emerging challenges. The rapid evolution of this field necessitates a forward-looking approach, where identifying potential avenues for innovation and improvement is crucial. This foresight not only prepares us to adapt to upcoming technological shifts but also guides the research community toward addressing the most pertinent and impactful areas. In this context, we outline several promising directions for future research and development, each offering the potential to significantly enhance the efficiency of serving generative LLMs.

‚àô
 Development and Enhancement of Hardware Accelerators
Future progress in enhancing generative LLM serving efficiency could be significantly driven by the development and refinement of specialized hardware accelerators, complemented by a co-design approach that aligns hardware and software optimizations. For instance, integrating memory closer to processing units or optimizing chip architectures to better align with the data flow of LLM algorithms can lead to substantial reductions in latency and energy consumption. This approach has been exemplified in recent GPU advancements, like NVIDIA‚Äôs Hopper architecture (nvh, 2022), which demonstrates improvements in HBM and SRAM capacity, memory bandwidth, computing units and bisection bandwidth, directly benefiting the processing of LLMs. Continued innovation in this area could involve designing hardware that is inherently tuned to the computational patterns of generative LLMs, such as optimizing for the specific demands of attention mechanisms and tensor operations that are prevalent in these models, eventually influencing the design and implementation of LLM serving systems.

‚àô
 Efficient and Effective Decoding Algorithms
The development of more efficient decoding algorithms could substantially improve serving efficiency. Motivated by the demand for more resource-efficient ways to utilize the vast knowledge encapsulated within LLMs, future work could explore alternative approaches to the traditional auto-regressive methods and unlock the generation speed for real-time applications while maintaining the decoding quality. One promising direction is generalized speculative inference as it enables preserving the same generation quality. Specifically, the small speculative model can be generalized to any other forms of methods that can generate draft tokens more efficiently than LLMs, such as knowledge retriever and user-defined functions (Miao et al., 2023a; Yang et al., 2023a). For example, some subsequent works arose recently, replacing the draft model with early exiting (Yang et al., 2023b; Zhang et al., 2023e; Bae et al., 2023; Hooper et al., 2023) or non-autoregressive decoding (Ge et al., 2022; Fu et al., 2023). In summary, the development of efficient decoding algorithms like speculative decoding coupled with the underlying system optimizations represents a significant opportunity to enhance the serving efficiency of generative LLMs.

‚àô
 Long Context/Sequence Scenarios Optimization
As the application of LLMs continues to expand into more sophisticated scenarios, the demand for processing longer contexts or sequences is steadily growing. Serving LLMs with long-sequence workloads requires resolving the challenges from both the algorithm and system sides. In terms of LLMs, they often suffer from length generalization failure when sequences get longer than what was observed during training (Press et al., 2021) even enabling relative positional encoding (Chen et al., 2023b) or after fine-tuning on longer corpora (Bai et al., 2023). Even for some models that claim to support ultra-long contexts, studies have found that they encounter a situation of ‚Äúloss in the middle‚Äù (Liu et al., 2023d). Current approaches attempt to alleviate such limitations by reducing the computational sequence length while preserving relevant information, such as retrieval augmentation (Xu et al., 2023b), sequence compression (Jiang et al., 2023c) and caching (Gim et al., 2023). For the LLM serving systems, longer sequence brings critical challenges, including more memory consumption and access of KV cache and quadratic increasing computational complexity of self-attention.

‚àô
 Investigating Alternative Architectures
Although Transformer models and self-attention mechanisms currently dominate the landscape of LLMs, exploring alternative architectures is a promising direction for future research. The field of DL has historically seen a constant alternation of dominant architectures, with each new paradigm shift bringing about significant advancements. Given this trend, it‚Äôs important to consider other architectural approaches that could offer distinct advantages, especially for improved computational efficiency. For instance, some recent studies explore attention-free methods (Bozic et al., 2023), using pure MLP (Multi-Layer Perceptron) architectures to replace attention mechanisms. The evolution of DNN model architecture is not only a natural progression, but also a necessary exploration to uncover more efficient and effective ways of structuring LLMs.

‚àô
 Exploration of Deployment in Complex Environments
As the application of LLMs expands, a crucial future direction involves exploring and optimizing their deployment across various complex environments. This exploration goes beyond traditional cloud-based deployments to include scenarios like edge computing, hybrid computing (combining cloud and edge computing), decentralized computing, and the utilization of more affordable resources like spot instances. Each of these environments presents unique challenges and opportunities for LLM serving. For instance, edge computing allows for faster response times and reduced bandwidth usage by processing data closer to the source, but it also poses challenges in terms of limited computational resources and storage capacity. Hybrid computing (Qualcomm, 2023) offers a balanced approach but requires advanced management to distribute computational tasks efficiently. Decentralized computing presents a promising avenue for crowdsourcing computational resources, but it also brings additional considerations regarding data privacy and security (Zhang et al., 2023a; Lu et al., 2023). LLM serving over preemptive resources (Miao et al., 2024) can significantly reduce monetary costs but requires fault tolerance mechanisms to handle their inherent unpredictability and variability, ensuring consistent performance and system reliability. Successfully navigating the challenges from these complex environments will be key for more robust, scalable, and efficient LLM applications.

‚àô
 Automatic Adaptation to Specific Requirements
The diverse application-specific requirements create a wide range of innovative LLM serving optimization opportunities, such as parameter-efficient fine-tuning (Zhou et al., 2022b; Sheng et al., 2023a; Chen et al., 2023c), retrieval from external vector storage (Borgeaud et al., 2022), online learning and knowledge updates, multi-modal workloads, and chaining together different LLMs‚Äô capabilities (Wu et al., 2023a). These unique challenges also demand automatic and smooth integration of LLM serving techniques into existing IT infrastructures by extending the optimization space to the whole LLM lifetime, including data acquisition and processing, AutoML (Tornede et al., 2023) and model management (Nagrecha and Kumar, 2023), resource allocations, and performance monitoring.

8.Conclusion
Efficient LLM serving is a fundamental step towards democratizing access to advanced AI technologies. This survey aims to provide researchers, practitioners, and developers with a comprehensive understanding of the existing methodologies, enabling them to make informed decisions when deploying LLMs in real-world environments. By consolidating the latest research findings on algorithms and systems, this survey paper hopes to accelerate progress and foster innovation in the pursuit of highly efficient LLM serving solutions.
