{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('long-context.txt', 'r') as f:\n",
    "    input = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "llm = LLM(model=\"THUDM/chatglm3-6b\", trust_remote_code=True)\n",
    "# llm = LLM(model=\"THUDM/chatglm3-6b\", trust_remote_code=True, enable_chunked_prefill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = llm.get_tokenizer()\n",
    "tokens = tokenizer.encode(input)\n",
    "tokens += tokens\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "st = time.time()\n",
    "_, req_token_time, parallel_info = llm.generate(None, sampling_params, prompt_token_ids=[tokens[:1000]] * 4)\n",
    "for req in req_token_time:\n",
    "    print((np.array(req_token_time[req]) - st) * 1000000)\n",
    "print(parallel_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttft_dict = {req: 0 for req in req_token_time}\n",
    "for req in req_token_time:\n",
    "    ttft_dict[req] = (req_token_time[req][0] - st) * 1000000\n",
    "ttft_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVG = 5\n",
    "N_REQ = 2\n",
    "prefill_list = []\n",
    "parallel_array = []\n",
    "for token_len in range(500, 8001, 500):\n",
    "    avg_prefill = 0\n",
    "    for i in range(AVG):\n",
    "        st = time.time()\n",
    "        _, req_token_time, parallel_info = llm.generate(None, sampling_params, prompt_token_ids=[tokens[:token_len]] * N_REQ)\n",
    "        for req in req_token_time:\n",
    "            avg_prefill += (req_token_time[req][0] - st) * 1000000\n",
    "    prefill_list.append(avg_prefill/AVG/N_REQ)\n",
    "print(prefill_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
